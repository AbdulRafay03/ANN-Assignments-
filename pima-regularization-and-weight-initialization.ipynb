{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":482,"sourceType":"datasetVersion","datasetId":228}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shaikhabdulrafay03/pima-regularization-and-weight-initialization?scriptVersionId=166580110\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-10T08:45:18.18555Z","iopub.execute_input":"2024-03-10T08:45:18.186041Z","iopub.status.idle":"2024-03-10T08:45:18.210014Z","shell.execute_reply.started":"2024-03-10T08:45:18.186008Z","shell.execute_reply":"2024-03-10T08:45:18.208688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.212275Z","iopub.execute_input":"2024-03-10T08:45:18.212958Z","iopub.status.idle":"2024-03-10T08:45:18.242688Z","shell.execute_reply.started":"2024-03-10T08:45:18.212923Z","shell.execute_reply":"2024-03-10T08:45:18.241317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"DataSet Information:\")\nprint(\"-\" * 30)\nprint('DF Shape: ' ,df.shape)\nprint('number of Columns: ' ,len(df.columns))\nprint('number of Observations: ' ,len(df))\nprint('Number of values in df: ' , df.count().sum())\nprint('Total Number of Missing values in df: ' , df.isna().sum().sum())\nprint('percentage of Missing values : ' ,  \"{:.2f}\".format(df.isna().sum().sum()/df.count().sum() *100),'%')\nprint('Total Number of Duplicated records in df : ' , df.duplicated().sum().sum())\nprint('percentage of Duplicated values : ' ,  \"{:.2f}\".format(df.duplicated().sum().sum()/df.count().sum() *100),'%')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.245146Z","iopub.execute_input":"2024-03-10T08:45:18.245557Z","iopub.status.idle":"2024-03-10T08:45:18.27254Z","shell.execute_reply.started":"2024-03-10T08:45:18.245527Z","shell.execute_reply":"2024-03-10T08:45:18.27015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.274154Z","iopub.execute_input":"2024-03-10T08:45:18.274783Z","iopub.status.idle":"2024-03-10T08:45:18.289837Z","shell.execute_reply.started":"2024-03-10T08:45:18.274748Z","shell.execute_reply":"2024-03-10T08:45:18.288027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.29344Z","iopub.execute_input":"2024-03-10T08:45:18.294454Z","iopub.status.idle":"2024-03-10T08:45:18.342843Z","shell.execute_reply.started":"2024-03-10T08:45:18.294397Z","shell.execute_reply":"2024-03-10T08:45:18.34141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.344945Z","iopub.execute_input":"2024-03-10T08:45:18.345435Z","iopub.status.idle":"2024-03-10T08:45:18.362563Z","shell.execute_reply.started":"2024-03-10T08:45:18.345391Z","shell.execute_reply":"2024-03-10T08:45:18.361055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Outcome'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.363952Z","iopub.execute_input":"2024-03-10T08:45:18.365027Z","iopub.status.idle":"2024-03-10T08:45:18.378702Z","shell.execute_reply.started":"2024-03-10T08:45:18.36498Z","shell.execute_reply":"2024-03-10T08:45:18.377317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler()\ny = df.pop('Outcome')\nx , y = ros.fit_resample(df , y)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.37996Z","iopub.execute_input":"2024-03-10T08:45:18.380397Z","iopub.status.idle":"2024-03-10T08:45:18.400361Z","shell.execute_reply.started":"2024-03-10T08:45:18.380362Z","shell.execute_reply":"2024-03-10T08:45:18.398449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.403735Z","iopub.execute_input":"2024-03-10T08:45:18.404662Z","iopub.status.idle":"2024-03-10T08:45:18.413178Z","shell.execute_reply.started":"2024-03-10T08:45:18.404621Z","shell.execute_reply":"2024-03-10T08:45:18.411925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\nprint(\"Train set:\", X_train.shape, y_train.shape)\nprint(\"Test set:\", X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:45:18.415184Z","iopub.execute_input":"2024-03-10T08:45:18.416677Z","iopub.status.idle":"2024-03-10T08:45:18.430485Z","shell.execute_reply.started":"2024-03-10T08:45:18.41655Z","shell.execute_reply":"2024-03-10T08:45:18.429308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning Analysis\n\n## Methodology \n\n- Hyperparameter Variations:\n - Trying different activation functions\n - Adjusting the Dropout Rates\n - Implementing L1L2 Regularization\n - Implementing GlorotNormal fro weight initializtion\n - Keeping the number of epochs constant with implemented early stopping\n- I will be using StratifiedKFold for training\n\n## Evaluation Metrics for Hyperparameter Tuning\n- Training duration (number of epochs before early stopping , time in seconds)\n- F1,Recall, Precision and Accuracy and Test Loss\t\n \n\n\n| Experiment Number | Activation Functions      | Dropout Value | Regularized (Yes/No) | Weight/Bias Initialization (Yes/No) |\n|-------------------|---------------------------|---------------|----------------------|--------------------------------------|\n| 1                 | sigmoid, tanh             | 0.3           | Yes                  | Yes                                  |\n| 2                 | ReLU, Leaky ReLU          | 0.3           | No                   | No                                   |\n| 3                 | Leaky ReLU, ELU           | 0.7           | Yes                  | Yes                                  |\n| 4                 | sigmoid, ReLU             | 0.7           | No                   | No                                   |\n\n","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tensorflow import keras\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import layers, models, regularizers, initializers\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.layers import LeakyReLU\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T11:31:41.112904Z","iopub.execute_input":"2024-03-10T11:31:41.113331Z","iopub.status.idle":"2024-03-10T11:31:41.122609Z","shell.execute_reply.started":"2024-03-10T11:31:41.1133Z","shell.execute_reply":"2024-03-10T11:31:41.120678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_layers_af = ['tanh' , keras.layers.LeakyReLU(alpha=0.01) , 'elu' , 'relu']\noutput_layer_af = ['sigmoid' , 'relu' , keras.layers.LeakyReLU(alpha=0.01) , 'sigmoid']\ndropouts = [0.3 , 0.3 , 0.7 , 0.7]\nregularization = [True , True ,False , False]\nweight_init = [True , False ,True , False]\n\nf1 = []\npre = []\nre = []\n\ntimes = []\neps = []\ntrain_acc = []\ntest_loss = []\ntest_accu = []\n\nhistorys = []\nmodelss = []","metadata":{"execution":{"iopub.status.busy":"2024-03-10T11:31:59.592447Z","iopub.execute_input":"2024-03-10T11:31:59.593261Z","iopub.status.idle":"2024-03-10T11:31:59.604708Z","shell.execute_reply.started":"2024-03-10T11:31:59.593204Z","shell.execute_reply":"2024-03-10T11:31:59.603649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for hl , ol , dr , r , wi in zip(hidden_layers_af , output_layer_af , dropouts , regularization , weight_init):\n\n\n    if r == True :\n        reg = regularizers.l1_l2(l1=1e-5, l2=1e-4)\n    else: \n        reg = None\n    \n    if wi == True :\n        w_init = initializers.GlorotNormal()\n        b_init = initializers.Zeros()\n    else : \n        w_init = None\n        b_init = None\n        \n    model = models.Sequential([\n        layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n        layers.Dense(256, activation= hl , kernel_regularizer= reg,\n                     kernel_initializer=w_init , bias_initializer=b_init),\n        layers.BatchNormalization(),\n        layers.Dropout(dr),\n        layers.Dense(256, activation=hl , kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n                     kernel_initializer=w_init, bias_initializer=b_init),\n        layers.BatchNormalization(),\n        layers.Dropout(dr),\n        layers.Dense(512, activation=hl , kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n                     kernel_initializer=w_init, bias_initializer=b_init),\n        layers.BatchNormalization(),\n        layers.Dropout(dr),\n        layers.Dense(1, activation=ol),\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=5,\n        min_delta=0.001,\n        restore_best_weights=True,\n    )\n\n\n    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    t = []\n    e = []\n    ta = []\n    for train_indices, val_indices in k_fold.split(X_train, y_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_indices], X_train.iloc[val_indices]\n        y_train_fold, y_val_fold = y_train.iloc[train_indices], y_train.iloc[val_indices]\n\n        start_time = time.time()\n        history = model.fit(\n            X_train_fold, y_train_fold,\n            validation_data=(X_val_fold, y_val_fold),\n            batch_size=256,\n            epochs=200,\n            callbacks=[early_stopping],\n        )\n        end_time = time.time()\n        training_time = end_time - start_time\n        t.append(training_time)\n        e.append(len(history.history['loss']))\n        ta.append(history.history['accuracy'][-1])\n    \n    times.append(sum(t) / float(len(t)))\n    eps.append(sum(e) / float(len(e)))\n    train_acc.append(sum(ta) / float(len(ta)))\n\n    eval = model.evaluate(X_test , y_test)\n    print('Test Loss and Accuracy : ' , eval)\n    test_loss.append(eval[0])\n    test_accu.append(eval[1])\n    \n    predictions = model.predict(X_test)\n    prediction_labels = []\n\n    for prediction in predictions:\n      prediction_labels.append(np.argmax(prediction))\n    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, prediction_labels, average='binary')\n    f1.append(f1_score)\n    pre.append(precision)\n    re.append(recall)\n    modelss.append(model)\n    historys.append(history)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T11:32:00.023823Z","iopub.execute_input":"2024-03-10T11:32:00.024272Z","iopub.status.idle":"2024-03-10T11:32:46.067141Z","shell.execute_reply.started":"2024-03-10T11:32:00.024236Z","shell.execute_reply":"2024-03-10T11:32:46.065696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Desicion Boundaries","metadata":{}},{"cell_type":"code","source":"def db(model, ax , i):\n    scaler = StandardScaler()\n    X_train_std = scaler.fit_transform(X_train)\n\n    # Apply PCA to reduce dimensions to 2\n    pca = PCA(n_components=2)\n    train_images_pca = pca.fit_transform(X_train_std)\n\n    # Define meshgrid for decision boundary\n    x_min, x_max = train_images_pca[:, 0].min() - 0.5, train_images_pca[:, 0].max() + 0.5\n    y_min, y_max = train_images_pca[:, 1].min() - 0.5, train_images_pca[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n\n    meshgrid_points = np.c_[xx.ravel(), yy.ravel()]\n    meshgrid_points_original = pca.inverse_transform(meshgrid_points)\n\n    # Predict on meshgrid points\n    predictions = model.predict(meshgrid_points_original)\n    Z = np.argmax(predictions, axis=1)\n    Z = Z.reshape(xx.shape)\n\n    # Plot decision boundary\n    ax.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title(f'Model {i}')\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n\n\ndb(modelss[0], axes[0, 0] , 1)\ndb(modelss[1], axes[0, 1] , 2)\ndb(modelss[2] , axes[1, 0] ,3)\ndb(modelss[3] , axes[1, 1] ,4)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T11:01:50.831718Z","iopub.execute_input":"2024-03-10T11:01:50.832322Z","iopub.status.idle":"2024-03-10T11:08:12.964308Z","shell.execute_reply.started":"2024-03-10T11:01:50.832278Z","shell.execute_reply":"2024-03-10T11:08:12.962612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"output_layer_af = ['sigmoid' , 'relu' , \"LeakyRelu\" , 'sigmoid']\nhidden_layers_af = ['tanh' , 'LeakyRelu', 'elu' , 'relu']\n\nd = {\n'Hidden Layer AF' : hidden_layers_af,\n'Output Layer AF' : output_layer_af,\n'DropOut Rate' : dropouts,\n'Regulariztion' : regularization,\n'Weights Initialzation' : weight_init,\n'Train time' : times,\n'Epochs' : eps,\n'Train Accuracy' : train_acc,\n'Test Loss' : test_loss,\n'Test Accuracy' : test_accu,\n'F1 Score' : f1,\n'Precsion' : pre,\n'Recall' : re\n}\n\ndd = pd.DataFrame(d)\ndd\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T11:41:27.194489Z","iopub.execute_input":"2024-03-10T11:41:27.194965Z","iopub.status.idle":"2024-03-10T11:41:27.22959Z","shell.execute_reply.started":"2024-03-10T11:41:27.194935Z","shell.execute_reply":"2024-03-10T11:41:27.22772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results:\n#### Experiment 1:\n- Shows underfitting\n- regulariztion and initializatuion are preventing overfitting\n\n#### Experiment 2:\n- Shows overfitting\n- No initializatuion are causing overfitting\n\n#### Experiment 3:\n- Shows underfitting\n- high dropout rate causing underfitting \n\n#### Experiment 1:\n- Shows underfitting\n- high dropout rate causing underfitting \n \n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers, regularizers, initializers\nfrom scipy.stats import uniform, randint\nimport numpy as np\n\n# Assuming X_train, y_train, X_test, y_test are your data\n# Adjust the hyperparameter lists based on your requirements\nhidden_layers_af = ['tanh', keras.layers.LeakyReLU(alpha=0.01), 'elu', 'relu']\noutput_layer_af = ['sigmoid', 'relu', keras.layers.LeakyReLU(alpha=0.01), 'sigmoid']\ndropouts = [0.3, 0.3, 0.7, 0.7]\nregularization = [True, False]\nweight_init = [True, False]\nbatch_sizes = [32, 64, 128, 256]\nlearning_rates = uniform(loc=0.0001, scale=0.1)\n\n# Define the number of folds for cross-validation\nn_splits = 5\nk_fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Create a dictionary of hyperparameters and their possible values\nparam_dist = {\n    'hl': hidden_layers_af,\n    'ol': output_layer_af,\n    'dr': dropouts,\n    'reg': regularization,\n    'w_init': weight_init,\n    'batch_size': batch_sizes,\n    'learning_rate': learning_rates\n}\n\n\nrandom_search_results = []\n\n\nn_iter = 10\nfor _ in range(n_iter):\n   \n    hyperparams = {key: np.random.choice(values) if key != 'learning_rate' else values.rvs() for key, values in param_dist.items()}\n\n    accuracies = []\n\n    for train_indices, val_indices in k_fold.split(X_train, y_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_indices], X_train.iloc[val_indices]\n        y_train_fold, y_val_fold = y_train.iloc[train_indices], y_train.iloc[val_indices]\n\n        model = Sequential([\n            layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n            layers.Dense(256, activation=hyperparams['hl'], kernel_regularizer=(regularizers.l1_l2(l1=1e-5, l2=1e-4) if hyperparams['reg'] else None),\n                         kernel_initializer=(initializers.GlorotNormal() if hyperparams['w_init'] else initializers.Zeros()),\n                         bias_initializer=initializers.Zeros()),\n            layers.BatchNormalization(),\n            layers.Dropout(hyperparams['dr']),\n            layers.Dense(256, activation=hyperparams['hl'], kernel_regularizer=(regularizers.l1_l2(l1=1e-5, l2=1e-4) if hyperparams['reg'] else None),\n                         kernel_initializer=(initializers.GlorotNormal() if hyperparams['w_init'] else initializers.Zeros()),\n                         bias_initializer=initializers.Zeros()),\n            layers.BatchNormalization(),\n            layers.Dropout(hyperparams['dr']),\n            layers.Dense(512, activation=hyperparams['hl'], kernel_regularizer=(regularizers.l1_l2(l1=1e-5, l2=1e-4) if hyperparams['reg'] else None),\n                         kernel_initializer=(initializers.GlorotNormal() if hyperparams['w_init'] else initializers.Zeros()),\n                         bias_initializer=initializers.Zeros()),\n            layers.BatchNormalization(),\n            layers.Dropout(hyperparams['dr']),\n            layers.Dense(1, activation=hyperparams['ol']),\n        ])\n\n        model.compile(\n            optimizer=Adam(learning_rate=hyperparams['learning_rate']),\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n\n        history = model.fit(\n            X_train_fold, y_train_fold,\n            validation_data=(X_val_fold, y_val_fold),\n            batch_size=hyperparams['batch_size'],\n            epochs=200,\n            callbacks=[early_stopping],\n            verbose=0,\n        )\n\n        y_pred_val = (model.predict(X_val_fold) > 0.5).astype(int)\n        accuracy_val = accuracy_score(y_val_fold, y_pred_val)\n        accuracies.append(accuracy_val)\n\n    avg_accuracy = sum(accuracies) / len(accuracies)\n\n    random_search_results.append((hyperparams['hl'], hyperparams['ol'], hyperparams['dr'], hyperparams['reg'],\n                                  hyperparams['w_init'], hyperparams['batch_size'], hyperparams['learning_rate'], avg_accuracy))\n\nbest_hyperparams = max(random_search_results, key=lambda x: x[-1])\n\nprint(\"Best Hyperparameters:\")\nprint(\"Hidden Layer Activation:\", best_hyperparams[0])\nprint(\"Output Layer Activation:\", best_hyperparams[1])\nprint(\"Dropout Rate:\", best_hyperparams[2])\nprint(\"Regularization:\", best_hyperparams[3])\nprint(\"Weight Initialization:\", best_hyperparams[4])\nprint(\"Batch Size:\", best_hyperparams[5])\nprint(\"Learning Rate:\", best_hyperparams[6])\nprint(\"Average Accuracy:\", best_hyperparams[7])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:16:16.809039Z","iopub.execute_input":"2024-03-10T10:16:16.809485Z","iopub.status.idle":"2024-03-10T10:42:52.837205Z","shell.execute_reply.started":"2024-03-10T10:16:16.809455Z","shell.execute_reply":"2024-03-10T10:42:52.835818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \nfor train_indices, val_indices in k_fold.split(X_train, y_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_indices], X_train.iloc[val_indices]\n        y_train_fold, y_val_fold = y_train.iloc[train_indices], y_train.iloc[val_indices]\n\n        model = Sequential([\n            layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n            layers.Dense(256, activation= keras.layers.LeakyReLU(alpha=0.01) , kernel_regularizer= None,\n                         kernel_initializer= initializers.Zeros(),\n                         bias_initializer=initializers.Zeros()),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n             layers.Dense(256, activation= keras.layers.LeakyReLU(alpha=0.01) , kernel_regularizer= None,\n                         kernel_initializer= initializers.Zeros(),\n                         bias_initializer=initializers.Zeros()),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n             layers.Dense(512, activation= keras.layers.LeakyReLU(alpha=0.01) , kernel_regularizer= None,\n                         kernel_initializer= initializers.Zeros(),\n                         bias_initializer=initializers.Zeros()),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),            \n            layers.BatchNormalization(),\n            layers.Dense(1, activation=hyperparams['ol']),\n        ])\n\n        model.compile(\n            optimizer=Adam(learning_rate=0.003518474982689479),\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n\n        history = model.fit(\n            X_train_fold, y_train_fold,\n            validation_data=(X_val_fold, y_val_fold),\n            batch_size=256,\n            epochs=200,\n            callbacks=[early_stopping],\n            verbose=0,\n        )\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:56.074078Z","iopub.execute_input":"2024-03-10T10:55:56.075328Z","iopub.status.idle":"2024-03-10T10:56:28.486421Z","shell.execute_reply.started":"2024-03-10T10:55:56.07525Z","shell.execute_reply":"2024-03-10T10:56:28.484903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\n\npca = PCA(n_components=2)\ntrain_images_pca = pca.fit_transform(X_train_std)\n\nx_min, x_max = train_images_pca[:, 0].min() - 0.5, train_images_pca[:, 0].max() + 0.5\ny_min, y_max = train_images_pca[:, 1].min() - 0.5, train_images_pca[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n\nmeshgrid_points = np.c_[xx.ravel(), yy.ravel()]\nmeshgrid_points_original = pca.inverse_transform(meshgrid_points)\n\npredictions = model.predict(meshgrid_points_original)\nZ = np.argmax(predictions, axis=1)\nZ = Z.reshape(xx.shape)\n\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Decision Boundary')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T11:00:41.234424Z","iopub.execute_input":"2024-03-10T11:00:41.234936Z","iopub.status.idle":"2024-03-10T11:01:07.819482Z","shell.execute_reply.started":"2024-03-10T11:00:41.234901Z","shell.execute_reply":"2024-03-10T11:01:07.817372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}