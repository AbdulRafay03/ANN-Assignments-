{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shaikhabdulrafay03/hyperparameter-tuning-and-desicion-boundary?scriptVersionId=166580212\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-03T12:00:44.449747Z","iopub.execute_input":"2024-03-03T12:00:44.450185Z","iopub.status.idle":"2024-03-03T12:00:44.472014Z","shell.execute_reply.started":"2024-03-03T12:00:44.450153Z","shell.execute_reply":"2024-03-03T12:00:44.469904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\ntest = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:45.549337Z","iopub.execute_input":"2024-03-03T12:00:45.549774Z","iopub.status.idle":"2024-03-03T12:00:52.738933Z","shell.execute_reply.started":"2024-03-03T12:00:45.549741Z","shell.execute_reply":"2024-03-03T12:00:52.737816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\npixels = train.iloc[0, 1:]  \nimage = pixels.values.reshape(28, 28)\n\nplt.imshow(image, cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:52.740814Z","iopub.execute_input":"2024-03-03T12:00:52.741237Z","iopub.status.idle":"2024-03-03T12:00:52.945539Z","shell.execute_reply.started":"2024-03-03T12:00:52.741206Z","shell.execute_reply":"2024-03-03T12:00:52.944748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"DataSet Information:\")\nprint(\"-\" * 30)\nprint('DF Shape: ' ,train.shape)\nprint('number of Columns: ' ,len(train.columns))\nprint('number of Observations: ' ,len(train))\nprint('Number of values in train: ' , train.count().sum())\nprint('Total Number of Missing values in train: ' , train.isna().sum().sum())\nprint('percentage of Missing values : ' ,  \"{:.2f}\".format(train.isna().sum().sum()/train.count().sum() *100),'%')\nprint('Total Number of Duplicated records in train : ' , train.duplicated().sum().sum())\nprint('percentage of Duplicated values : ' ,  \"{:.2f}\".format(train.duplicated().sum().sum()/train.count().sum() *100),'%')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:52.946589Z","iopub.execute_input":"2024-03-03T12:00:52.947529Z","iopub.status.idle":"2024-03-03T12:00:55.365988Z","shell.execute_reply.started":"2024-03-03T12:00:52.947495Z","shell.execute_reply":"2024-03-03T12:00:55.364925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:55.368846Z","iopub.execute_input":"2024-03-03T12:00:55.369279Z","iopub.status.idle":"2024-03-03T12:00:56.654879Z","shell.execute_reply.started":"2024-03-03T12:00:55.369242Z","shell.execute_reply":"2024-03-03T12:00:56.653355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['label'].value_counts().sort_index(ascending=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:56.656324Z","iopub.execute_input":"2024-03-03T12:00:56.656674Z","iopub.status.idle":"2024-03-03T12:00:56.677706Z","shell.execute_reply.started":"2024-03-03T12:00:56.65663Z","shell.execute_reply":"2024-03-03T12:00:56.675274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\ny = train.pop('label')\ny_train = keras.utils.to_categorical(y, 10)\n\ny_test = test.pop('label')\ny_test = keras.utils.to_categorical(y_test, 10)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:56.67985Z","iopub.execute_input":"2024-03-03T12:00:56.680405Z","iopub.status.idle":"2024-03-03T12:00:56.747296Z","shell.execute_reply.started":"2024-03-03T12:00:56.680358Z","shell.execute_reply":"2024-03-03T12:00:56.745354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalize data\nX_train = train/255\ntest = test / 255","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:56.749524Z","iopub.execute_input":"2024-03-03T12:00:56.750006Z","iopub.status.idle":"2024-03-03T12:00:57.058641Z","shell.execute_reply.started":"2024-03-03T12:00:56.749967Z","shell.execute_reply":"2024-03-03T12:00:57.057137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming 'your_data' is your DataFrame and 'your_labels' is the corresponding label column\nX_test, X_val, y_test, y_val = train_test_split(test, y_test, test_size=0.5, random_state=42)\n\n# Print the shapes of the resulting sets\nprint(\"Train set:\", X_train.shape, y_train.shape)\nprint(\"Validation set:\", X_val.shape, y_val.shape)\nprint(\"Test set:\", X_test.shape, y_test.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:00:57.060095Z","iopub.execute_input":"2024-03-03T12:00:57.060441Z","iopub.status.idle":"2024-03-03T12:00:57.693009Z","shell.execute_reply.started":"2024-03-03T12:00:57.060411Z","shell.execute_reply":"2024-03-03T12:00:57.692111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning Analysis\n\n## Methodology \n\n- Hyperparameter Variations:\n - Varying the number of layers\n - Trying different activation functions\n - Adjusting the learning rate\n - Experimenting with batch sizes\n - Keeping the number of epochs constant with implemented early stopping\n\n\n## Evaluation Metrics for Hyperparameter Tuning\n- Training duration (number of epochs before early stopping , time in seconds)\n- Noise levels in accuracy and loss graphs (on both training and validation sets)\n- Test Loss and accuracy\n| `Experiment Number`  | `learning_rate` | `batch_size` | `Number of Layers` | `Activation Functions` | `Batch Normalization & Dropout`  |\n|----------------------|-----------------|--------------|--------------------|------------------------|----------------------------------|\n| 1                    | 0.05            | 32           | 1                  | relu, sigmoid          | Yes                              |\n| 2                    | 0.05            | 16           | 1                  | relu, softmax          | No                               |\n| 3                    | 0.05            | 128          | 3                  | tanh, sigmoid          | Yes                              |\n| 4                    | 0.02            | 256          | 3                  | tanh, softmax          | No                               |\n| 5                    | 0.2             | 512          | 5                  | relu, softmax          | Yes                              |\n| 6                    | 1.0             | 128          | 5                  | leakyrelu, sigmoid     | No                               |\n| 7                    | 0.9             | 2048         | 4                  | relu, softmax          | Yes                              |\n| 8                    | 0.99            | 4096         | 4                  | tanh, sigmoid          | No                               |\n\n","metadata":{}},{"cell_type":"code","source":"times = []\neps = []\ntrain_acc = []\ntest_loss = []\ntest_accu = []","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:43:39.192921Z","iopub.execute_input":"2024-03-03T08:43:39.193252Z","iopub.status.idle":"2024-03-03T08:43:39.198624Z","shell.execute_reply.started":"2024-03-03T08:43:39.193203Z","shell.execute_reply":"2024-03-03T08:43:39.197391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 1\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(10, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=0.05), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=32,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:43:39.203489Z","iopub.execute_input":"2024-03-03T08:43:39.204009Z","iopub.status.idle":"2024-03-03T08:45:02.422647Z","shell.execute_reply.started":"2024-03-03T08:43:39.203979Z","shell.execute_reply":"2024-03-03T08:45:02.421481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.Dense(256, activation='relu' , input_shape=[X_train.shape[1]]),\n    layers.Dense(10, activation='softmax'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=0.05), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=16,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:45:02.424049Z","iopub.execute_input":"2024-03-03T08:45:02.424388Z","iopub.status.idle":"2024-03-03T08:47:26.773678Z","shell.execute_reply.started":"2024-03-03T08:45:02.42436Z","shell.execute_reply":"2024-03-03T08:47:26.772637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 3\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='tanh'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='tanh'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='tanh'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(10, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=0.05), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=128,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:47:26.775176Z","iopub.execute_input":"2024-03-03T08:47:26.775548Z","iopub.status.idle":"2024-03-03T08:48:05.377328Z","shell.execute_reply.started":"2024-03-03T08:47:26.775512Z","shell.execute_reply":"2024-03-03T08:48:05.37617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 4\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.Dense(256, activation='tanh' , input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='tanh'),\n    layers.Dense(256, activation='tanh'),\n    layers.Dense(10, activation='softmax'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=0.02), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=256,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\nhistory_df = pd.DataFrame(history.history)\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:48:05.379047Z","iopub.execute_input":"2024-03-03T08:48:05.380016Z","iopub.status.idle":"2024-03-03T08:48:28.967411Z","shell.execute_reply.started":"2024-03-03T08:48:05.379974Z","shell.execute_reply":"2024-03-03T08:48:28.966271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 5\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=0.2), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:48:28.968655Z","iopub.execute_input":"2024-03-03T08:48:28.968971Z","iopub.status.idle":"2024-03-03T08:49:21.996665Z","shell.execute_reply.started":"2024-03-03T08:48:28.968944Z","shell.execute_reply":"2024-03-03T08:49:21.995524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 6\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.Dense(256, activation='relu' , input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(10, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=1), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=128,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:49:21.997985Z","iopub.execute_input":"2024-03-03T08:49:21.998297Z","iopub.status.idle":"2024-03-03T08:49:51.832508Z","shell.execute_reply.started":"2024-03-03T08:49:21.99827Z","shell.execute_reply":"2024-03-03T08:49:51.831415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 7\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=0.9), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=2048,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:49:51.834395Z","iopub.execute_input":"2024-03-03T08:49:51.835306Z","iopub.status.idle":"2024-03-03T08:50:37.43204Z","shell.execute_reply.started":"2024-03-03T08:49:51.835274Z","shell.execute_reply":"2024-03-03T08:50:37.430896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiment Number: 8\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\nstart_time = time.time()\nmodel = keras.Sequential([\n    layers.Dense(256, activation='tanh' , input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='tanh'),\n    layers.Dense(256, activation='tanh'),\n    layers.Dense(256, activation='tanh'),\n    layers.Dense(10, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer = Adam(learning_rate=0.99), \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=4096,\n    epochs=200,\n    callbacks=[early_stopping],\n)\nend_time = time.time()\ntraining_time = end_time - start_time\ntimes.append(training_time)\neps.append(len(history.history['loss']))\ntrain_acc.append(history.history['accuracy'][-1])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\neval = model.evaluate(X_test , y_test)\nprint('Training Time: ' ,training_time)\nprint('Test Loss and Accuracy : ' , eval)\ntest_loss.append(eval[0])\ntest_accu.append(eval[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T08:50:37.43372Z","iopub.execute_input":"2024-03-03T08:50:37.434093Z","iopub.status.idle":"2024-03-03T08:50:58.558261Z","shell.execute_reply.started":"2024-03-03T08:50:37.434062Z","shell.execute_reply":"2024-03-03T08:50:58.557079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"| `Experiment Number`  | `learning_rate` | `batch_size` | `Number of Layers` | `Activation Functions` | `Batch Normalization & Dropout`  |\n|----------------------|-----------------|--------------|--------------------|------------------------|----------------------------------|\n| 1                    | 0.05            | 32           | 1                  | relu, sigmoid          | Yes                              |\n| 2                    | 0.05            | 2            | 1                  | relu, softmax          | No                               |\n| 3                    | 0.05            | 128          | 3                  | tanh, sigmoid          | Yes                              |\n| 4                    | 0.02            | 256          | 3                  | tanh, softmax          | No                               |\n| 5                    | 0.2             | 512          | 5                  | relu, softmax          | Yes                              |\n| 6                    | 1.0             | 128          | 5                  | leakyrelu, sigmoid     | No                               |\n| 7                    | 0.9             | 2048         | 4                  | relu, softmax          | Yes                              |\n| 8                    | 0.99            | 4096         | 4                  | tanh, sigmoid          | No                               |\n","metadata":{}},{"cell_type":"code","source":"dict = {\n    'Experiment Number' : [1,2,3,4,5,6,7,8],\n    'training time in seconds' : times,\n    'Number of Epochs' : eps,\n    'Training Accuracy' : train_acc,\n    'Test Loss' : test_loss,\n    'Test Accuracy ': test_accu\n}\n\ndd = pd.DataFrame(dict)\ndd","metadata":{"execution":{"iopub.status.busy":"2024-03-03T09:06:32.477661Z","iopub.execute_input":"2024-03-03T09:06:32.478318Z","iopub.status.idle":"2024-03-03T09:06:32.49717Z","shell.execute_reply.started":"2024-03-03T09:06:32.478277Z","shell.execute_reply":"2024-03-03T09:06:32.4959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\n- Experiment 1:\n  - Good accuracy.\n  - Big difference in loss and accuracy between training and validation datasets.\n  - Generalized model.\n  - Long training time due to more epochs.\n\n- Experiment 2:\n  - Low accuracy.\n  - Noisy graphs of training loss and accuracy.\n  - Underfitting observed.\n  - Long training time.\n\n- Experiment 3:\n  - Low accuracy.\n  - Underfitting.\n  - Shorter training time.\n  - Less noisy graph.\n\n- Experiment 4:\n  - Moderate accuracy.\n  - Short training time.\n  - Graphs are a bit glitchy.\n\n- Experiment 5:\n  - Very glitchy graphs.\n  - Underfitting.\n  - Long training time.\n\n- Experiment 6:\n  - Underfitting.\n  - Short training time.\n  - Very glitchy.\n\n- Experiment 7:\n  - Underfitting.\n  - Short training time.\n  - Very noisy.\n\n- Experiment 8:\n  - Underfitting.\n  - Short training time.\n  - Very glitchy graphs.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\n- With the learning rate and the batch size, you have some control over:\n - How long it takes to train a model\n - How noisy the learning curves are\n - How small the loss becomes\n \n- Number of Layers:\n - Influences the model's complexity and learning capacity.\n - Impacts training time and potential overfitting.\n - Affects the depth of learning curves and the model's ability to capture intricate patterns.\n\n- Batch Normalization and Dropouts:\n\n - Batch Normalization helps in stabilizing and accelerating training.\n - Dropouts act as a regularization technique, preventing overfitting.\n - Together, they can influence training time, noise in learning curves, and the model's robustness.\n","metadata":{}},{"cell_type":"code","source":"\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# YOUR CODE HERE: define the model given in the diagram\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax'),\n])\n\nmodel.compile(\n    optimizer = 'adam', \n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"categorical_crossentropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\nprint('Test Loss and Accuracy : ' ,model.evaluate(X_test , y_test))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:11:57.452415Z","iopub.execute_input":"2024-03-03T12:11:57.4529Z","iopub.status.idle":"2024-03-03T12:13:13.685254Z","shell.execute_reply.started":"2024-03-03T12:11:57.452862Z","shell.execute_reply":"2024-03-03T12:13:13.684034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming X_train is a DataFrame\nX_train_array = X_train.to_numpy()  # Convert DataFrame to NumPy array\n\n# Flatten the input data\nX_train_flattened = X_train_array.reshape(X_train_array.shape[0], -1)\n\npca = PCA(n_components=2)\ntrain_images_pca = pca.fit_transform(X_train_flattened)\n\nx_min, x_max = train_images_pca[:, 0].min() - 50, train_images_pca[:, 0].max() + 50\ny_min, y_max = train_images_pca[:, 1].min() - 50, train_images_pca[:, 1].max() + 50\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 10),\n                     np.arange(y_min, y_max, 10))\n\nmeshgrid_points = np.c_[xx.ravel(), yy.ravel()]\nmeshgrid_points_original = pca.inverse_transform(meshgrid_points)\nmeshgrid_points_original = meshgrid_points_original.reshape(-1, 28, 28)\n\n# Flatten the meshgrid points\nmeshgrid_points_flattened = meshgrid_points_original.reshape(meshgrid_points_original.shape[0], -1)\n\npredictions = model.predict(meshgrid_points_flattened)\nZ = np.argmax(predictions, axis=1)\nZ = Z.reshape(xx.shape)\n\n# Plot decision boundary\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Decision Boundary')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T12:17:10.769337Z","iopub.execute_input":"2024-03-03T12:17:10.769835Z","iopub.status.idle":"2024-03-03T12:17:13.063594Z","shell.execute_reply.started":"2024-03-03T12:17:10.769798Z","shell.execute_reply":"2024-03-03T12:17:13.062233Z"},"trusted":true},"execution_count":null,"outputs":[]}]}